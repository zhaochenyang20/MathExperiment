# 1

```
1.1635
龙格库塔方法
ode45
4阶
```

```python
import numpy as np
from scipy.integrate import odeint

def dyt_dx(input_list, x):
    y, t = input_list
    dy_dx = t
    dt_dx = y * np.sin(x)
    return [dy_dx, dt_dx]

# 初始条件
yt_0 = [1, 0]

# 自变量范围
x = np.linspace(0, 1, 10000)

# 求解微分方程
y = odeint(dyt_dx, yt_0, x)

# 输出结果
print(round(y[-1, 0], 5))
```

# 2

$$
t = \frac{\overline{x}-\mu}{s/\sqrt{n}} \sim t(n-1) \\
\mu的置信区间为[15.440787, 17.359213]\\
\text{［mu,sigma, muci, sigmaci］=normfit(x,alpha)}
$$

```python
n=25;
mu=16.4;
s2=5.4;
alpha=0.05;
lower = mu-sqrt(s2)*tinv(1-alpha/2,n-1)/sqrt(n);
upper = mu+sqrt(s2)*tinv(1-alpha/2,n-1)/sqrt(n);
fprintf('置信区间为 [%f, %f]\n', lower, upper);
```

```python
def t_test(data_list, confidence_level, mu_0=None, test_method="double"):
    print("用于在方差未知的情况下估计均值或者对均值进行检验")
    assert test_method in ["left", "right", "double"]
    from scipy.stats import t
    import numpy as np
    n = 25
    sample_mean = 16.4
    print("样本均值: ", sample_mean)
    sample_std = np.sqrt(5.4)
    print("样本标准差: ", sample_std)
    center = mu_0 if mu_0 else sample_mean
    #! 假设检验需要传入假设的 mu_0，区间估计本身就是为了估计 mu_0，不用 mu_0
    if mu_0 != None:
        print("正在进行假设检验")
        t_statistic = (sample_mean - mu_0) / (sample_std / np.sqrt(n))
    else:
        print("正在进行区间估计")
    #! t_statistic 仅在计算 p 的时候使用，p 表示在 mu_0 正确时，sample_mean 比起当前更异常的概率
    if test_method == "double":
        print("正在计算双侧区间")
        #! alpha = 1 - confidence_level，故而 1 + confidence_level) / 2 = 1- alpha / 2
        t_value = t.ppf((1 + confidence_level) / 2, df=n - 1)
    else:
        print("正在计算单侧区间")
        t_value = t.ppf(confidence_level, df=n - 1)
    margin_error = t_value * sample_std / np.sqrt(n)
    lower_bound = center - margin_error
    upper_bound = center + margin_error
    if test_method == "double":
        #! 决定是否接受假设的时候，可以直接用 sample_mean 和 bound 比较，也可以用 t_statistic 和统计量（比如 u_{1- alpha / 2}）作对比
        print(f"Confidence Interval[{confidence_level}]:", lower_bound, "-", upper_bound)
        if mu_0 != None:
        #! p 和 confidence_level / alpha 没有任何关系，只有假设检验有 p 这个概念
            p_value = 2 * (1 - t.cdf(np.abs(t_statistic), df=n - 1))
    elif test_method == "right":
        #! 样本均值大于 right_bound 则拒绝假设
        print(f"Confidence Interval[{confidence_level}]:", "right_bound ", upper_bound)
        if mu_0 != None:
            p_value = 1 - t.cdf(t_statistic, df=n - 1)
    elif test_method == "left":
        #! 样本均值小于 left_bound 则拒绝假设
        print(f"Confidence Interval[{confidence_level}]:", "left_bound", lower_bound)
        if mu_0 != None:
            p_value = t.cdf(t_statistic, df=n - 1)
    if mu_0 != None:
        print(f"P-value: {p_value}")
        return p_value


t_test([], 0.95)
```

# 3

这道题的核心在于，题目是要求得到 11 个 k 的估计值，如果只用相邻两点的差作为加速度的话，就只有 10 个加速度了。所以选择了三点法计算加速度，然后端点采用相邻两点的差，这样就有了 11 个加速度了。从而可以计算均值，或者用线性回归。得到一个 list 的 k 后，用 t 检验即可。

```python
import numpy as np
from scipy.optimize import least_squares

# 加载数据集，假设已经有了一组v(t)与t的数据
t_data = np.arange(10, 21, 1)
v_data = np.array([190, 200, 210, 216, 225, 228, 231, 234, 239, 240, 246])
assert len(t_data) == len(v_data)
a_data = np.zeros(len(t_data))
for i in range(1, len(t_data) - 1):
    a_data[i] = (v_data[i + 1] - v_data[i - 1]) / 2
a_data[0] = v_data[1] - v_data[0]
a_data[len(t_data) - 1] = v_data[len(t_data) - 1] - v_data[len(t_data) - 2]

def objective(x):
    #! 构造残差函数，也即 f = g(t)，则残差为 f - g(t)
    k = x[0]
    F = np.zeros(len(t_data))
    for i in range(len(t_data)):
        v = v_data[i]
        a = a_data[i]
        t = t_data[i]
        m = 1200 -15 * t
        F[i] = a - (-k * v**2 / m + 40000 / m - 9.8)
    return F

x0 = [0.5]
#! x0 的选取影响很大，但是 tau 确实不该取 0
res = least_squares(objective, x0)
print(res.x)

def k_func(a, t, v):
    m = 1200 -15 * t
    k = (40000 - 9.8 * m - a * m) / v ** 2
    return k

k_list = []

for i in range(len(t_data)):
    k = k_func(a_data[i], t_data[i], v_data[i])
    k_list.append(k)

np.mean(k_list)

#! 上方采用了直接求均值法和最小二乘法

k_list.append(res.x[0])


def t_test(data_list, confidence_level, mu_0=None, test_method="double"):
    print("用于在方差未知的情况下估计均值或者对均值进行检验")
    assert test_method in ["left", "right", "double"]
    from scipy.stats import t
    import numpy as np
    n = len(data_list)
    sample_mean = np.mean(data_list)
    print("样本均值: ", sample_mean)
    sample_std = np.std(data_list, ddof=1)
    print("样本标准差: ", sample_std)
    center = mu_0 if mu_0 else sample_mean
    #! 假设检验需要传入假设的 mu_0，区间估计本身就是为了估计 mu_0，不用 mu_0
    if mu_0 != None:
        print("正在进行假设检验")
        t_statistic = (sample_mean - mu_0) / (sample_std / np.sqrt(n))
    else:
        print("正在进行区间估计")
    #! t_statistic 仅在计算 p 的时候使用，p 表示在 mu_0 正确时，sample_mean 比起当前更异常的概率
    if test_method == "double":
        print("正在计算双侧区间")
        #! alpha = 1 - confidence_level，故而 1 + confidence_level) / 2 = 1- alpha / 2
        t_value = t.ppf((1 + confidence_level) / 2, df=n - 1)
    else:
        print("正在计算单侧区间")
        t_value = t.ppf(confidence_level, df=n - 1)
    margin_error = t_value * sample_std / np.sqrt(n)
    lower_bound = center - margin_error
    upper_bound = center + margin_error
    if test_method == "double":
        #! 决定是否接受假设的时候，可以直接用 sample_mean 和 bound 比较，也可以用 t_statistic 和统计量（比如 u_{1- alpha / 2}）作对比
        print(f"Confidence Interval[{confidence_level}]:", lower_bound, "-", upper_bound)
        if mu_0 != None:
        #! p 和 confidence_level / alpha 没有任何关系，只有假设检验有 p 这个概念
            p_value = 2 * (1 - t.cdf(np.abs(t_statistic), df=n - 1))
    elif test_method == "right":
        #! 样本均值大于 right_bound 则拒绝假设
        print(f"Confidence Interval[{confidence_level}]:", "right_bound ", upper_bound)
        if mu_0 != None:
            p_value = 1 - t.cdf(t_statistic, df=n - 1)
    elif test_method == "left":
        #! 样本均值小于 left_bound 则拒绝假设
        print(f"Confidence Interval[{confidence_level}]:", "left_bound", lower_bound)
        if mu_0 != None:
            p_value = t.cdf(t_statistic, df=n - 1)
    if mu_0 != None:
        print(f"P-value: {p_value}")
        return p_value


t_test(k_list, 0.95, mu_0=0.5, test_method="double")
```
# 4

```python
import numpy as np
from scipy.optimize import linprog

# Define the objective function
c = np.array([67, 55, 72, 58, 150, 210, 190, 160])

# Define the inequality constraints
A_ub = np.array([[1, 0, 0, 1, 0, 0, 0, 0],])
b_ub = np.array([65,])

# Define the equality constraints
A_eq = np.array([
    [1, 1, 0, 0, 0, 0, 0, 0],
    [0, 0, 1, 1, 0, 0, 0, 0],
    [1, 0, 0, 1, -1, -1, 0, 0],
    [0, 1, 1, 0, 0, 0, -1, -1],
    [0, 0, 0, 0, 0, 1, 1, 0],
    [0, 0, 0, 0, 1, 0, 0, 1],
])
b_eq = np.array([
    90, 45, 0, 0, 55, 80
])

# Define the bounds
bounds = [(0, 90)] * 8

# Call the linprog function
res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)

print(res.fun)
```

优化成功，30360。

